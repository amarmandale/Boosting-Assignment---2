{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668a0472-951f-42f0-a72b-fc25a404bfaa",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "**Gradient Boosting Regression** is a machine learning technique used for regression tasks, where models are built sequentially, and each new model is trained to correct the residuals (errors) of the previous models. The goal of gradient boosting is to create a strong predictive model by combining several weak learners, typically decision trees. Each tree is fitted on the errors or residuals of the previous predictions, and the overall model improves iteratively using gradient descent to minimize a predefined loss function (e.g., mean squared error).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ac8d8-a890-4c11-a801-ec43d0a4e8a5",
   "metadata": {},
   "source": [
    "###  Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb8d4b14-502d-47ca-acb7-1167ce613e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.0408644744005826e-09\n",
      "R-squared: 0.9999999979795677\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class SimpleGradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the predictions as the mean of the target variable\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        predictions = np.full(y.shape, self.initial_prediction)\n",
    "        \n",
    "        # Train multiple decision trees (weak learners)\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the residuals (errors)\n",
    "            residuals = y - predictions\n",
    "            \n",
    "            # Train a decision tree on the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update predictions\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Store the tree model\n",
    "            self.models.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Initialize predictions with the mean value\n",
    "        predictions = np.full(X.shape[0], self.initial_prediction)\n",
    "        \n",
    "        # Sum the predictions of each tree in the ensemble\n",
    "        for tree in self.models:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Example dataset (simple synthetic dataset)\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 2.5, 3.5, 4.5, 5.5])\n",
    "\n",
    "# Instantiate the model\n",
    "gbr = SimpleGradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=2)\n",
    "gbr.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = gbr.predict(X)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c2d8d5-593c-4064-bb28-ca55d3ea5fa8",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\n",
    "To optimize performance, you can adjust hyperparameters using **grid search** or **random search**. Here's an example using `GridSearchCV`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0ced23-6544-4ed9-82fe-de19fe2fed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 100}\n",
      "Best Model Mean Squared Error: 1.9923772363239538e-16\n",
      "Best Model R-squared: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Instantiate the model\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X)\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(\"Best Model Mean Squared Error:\", mse)\n",
    "print(\"Best Model R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7352c0b-f17a-435f-9de2-640c8cfba90a",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "A **weak learner** in Gradient Boosting is a simple model that performs slightly better than random guessing. In Gradient Boosting, weak learners are typically shallow decision trees (trees with a small depth, often 1 or 2 levels). These weak learners are combined to form a strong model by focusing on correcting the errors of the previous learners.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind **Gradient Boosting** is that by training models sequentially and focusing on the mistakes made by previous models, we can progressively improve the model's accuracy. Each model corrects the residuals (errors) of the previous models, and gradient descent is used to minimize a loss function (e.g., mean squared error) iteratively.\n",
    "\n",
    "Instead of fitting a single large, complex model, gradient boosting builds a series of small, simple models (weak learners) that gradually reduce the error.\n",
    "\n",
    "---\n",
    "\n",
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Gradient Boosting builds an ensemble by training weak learners (usually shallow decision trees) sequentially. Each weak learner is trained to correct the errors (residuals) of the previous learners. The process is as follows:\n",
    "\n",
    "1. **First learner**: A weak learner (decision tree) is trained to predict the target values.\n",
    "2. **Residuals**: After the first learner makes predictions, the residuals (errors) between the predictions and the actual values are computed.\n",
    "3. **Next learners**: Each subsequent learner is trained to predict these residuals, and this process continues.\n",
    "4. **Aggregation**: The predictions of all weak learners are aggregated (summed) to produce the final prediction. A learning rate controls how much each learner's prediction contributes to the final model.\n",
    "\n",
    "---\n",
    "\n",
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting?\n",
    "\n",
    "1. **Initialization**: Start by predicting a constant value, typically the mean of the target values.\n",
    "   \n",
    "2. **Residual Calculation**: Compute the residuals (errors) between the true values and the predicted values. These residuals become the target for the next learner.\n",
    "\n",
    "3. **Model Training**: Fit a weak learner (e.g., a decision tree) to the residuals. This model is trained to minimize the residuals, effectively reducing the error.\n",
    "\n",
    "4. **Gradient Descent**: Gradient Boosting uses gradient descent to minimize a predefined loss function. Each new learner is fitted to the negative gradient of the loss function with respect to the current model's predictions.\n",
    "\n",
    "5. **Iteration**: Repeat this process for a set number of iterations (trees), with each new model trained to predict the residuals (errors) of the previous model.\n",
    "\n",
    "6. **Final Prediction**: The final prediction is a weighted sum of the initial prediction and all residual-correcting models. The learning rate determines the contribution of each learner to the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df73231-1dc8-4208-9b62-512c61284d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4a7ab-8039-4189-949a-f90f2e7dfa9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
